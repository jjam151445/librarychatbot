import os
import streamlit as st
import nest_asyncio

# Streamlitì—ì„œ ë¹„ë™ê¸° ì‘ì—…ì„ ìœ„í•œ ì´ë²¤íŠ¸ ë£¨í”„ ì„¤ì •
nest_asyncio.apply()

# LangChain components for RAG
from langchain_core.documents import Document
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_core.runnables.history import RunnableWithMessageHistory
from langchain.chains import create_retrieval_chain
from langchain.chains.combine_documents import create_stuff_documents_chain
from langchain.chains.history_aware_retriever import create_history_aware_retriever
from langchain_community.chat_message_histories.streamlit import StreamlitChatMessageHistory
from langchain_chroma import Chroma

# Workaround for Streamlit environment to use an in-memory SQLite for Chroma
__import__('pysqlite3')
import sys
sys.modules['sqlite3'] = sys.modules.pop('pysqlite3')


# Gemini API í‚¤ ì„¤ì • (Streamlit Secretsì—ì„œ ê°€ì ¸ì˜¤ê¸°)
try:
    # ì´ ë¶€ë¶„ì€ í™˜ê²½ì— ë”°ë¼ __initial_auth_tokenì„ ì‚¬ìš©í•˜ê±°ë‚˜, Streamlit secretsì—ì„œ GOOGLE_API_KEYë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.
    # ì‚¬ìš©ìì˜ ì›ë˜ ì½”ë“œë¥¼ ì¡´ì¤‘í•˜ì—¬ secrets ì‚¬ìš© êµ¬ë¬¸ì„ ìœ ì§€í•©ë‹ˆë‹¤.
    os.environ["GOOGLE_API_KEY"] = st.secrets["GOOGLE_API_KEY"]
except Exception as e:
    # ì´ í™˜ê²½ì—ì„œëŠ” API Keyê°€ ìë™ìœ¼ë¡œ ì£¼ì…ë˜ë¯€ë¡œ, ì˜¤ë¥˜ ëŒ€ì‹  ì•ˆë‚´ ë©”ì‹œì§€ë¥¼ í‘œì‹œí•©ë‹ˆë‹¤.
    st.info("ğŸ’¡ GOOGLE_API_KEYê°€ í™˜ê²½ ë³€ìˆ˜ë¡œë¶€í„° ìë™ ì„¤ì •ë©ë‹ˆë‹¤.")
    # ì‹¤ì œ Streamlit í™˜ê²½ì—ì„œëŠ” ì•„ë˜ ì½”ë“œë¥¼ ì‚¬ìš©í•˜ì—¬ í‚¤ ì„¤ì •ì„ ê°•ì œí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
    # st.error("âš ï¸ GOOGLE_API_KEYë¥¼ Streamlit Secretsì— ì„¤ì •í•´ì£¼ì„¸ìš”!")
    # st.stop()
    pass


# 1. íƒ„ì†Œ ë°°ì¶œ ë°ì´í„° ë¬¸ì„œ ìƒì„± (í•˜ë“œì½”ë”©ìœ¼ë¡œ ì‹¤ì œ ë°ì´í„° ë¶„ì„ í™˜ê²½ì„ ëª¨ë°©)
@st.cache_resource
def load_and_split_data():
    """íƒ„ì†Œ ë°°ì¶œëŸ‰ì— ëŒ€í•œ í•µì‹¬ ì‚¬ì‹¤ë“¤ì„ Document ê°ì²´ë¡œ ìƒì„±í•©ë‹ˆë‹¤."""
    
    # ì‹¤ì œ í™˜ê²½ì—ì„œëŠ” ì—¬ê¸°ì„œ CSVLoader, JSONLoader ë“±ì„ ì‚¬ìš©í•˜ì—¬ ë°ì´í„°ë¥¼ ë¡œë“œí•˜ê³  í•„ìš”í•˜ë©´ ë¶„í• í•©ë‹ˆë‹¤.
    # ì˜ˆì‹œ ë°ì´í„° (2023ë…„ ê°€ìƒ ë°ì´í„°)
    data_points = [
        ("2023ë…„ ì „ ì„¸ê³„ ì´ ì´ì‚°í™”íƒ„ì†Œ ë°°ì¶œëŸ‰ì€ ì•½ 368ì–µ í†¤ìœ¼ë¡œ ì¶”ì •ë©ë‹ˆë‹¤.", "Global Emissions Report 2023", 1),
        ("ê°€ì¥ ë§ì€ íƒ„ì†Œë¥¼ ë°°ì¶œí•˜ëŠ” êµ­ê°€ëŠ” ì¤‘êµ­ì´ë©°, ì´ëŠ” ì „ ì„¸ê³„ ë°°ì¶œëŸ‰ì˜ ì•½ 31%ë¥¼ ì°¨ì§€í•©ë‹ˆë‹¤.", "IEA 2023 Review", 2),
        ("ë¯¸êµ­ì€ ë‘ ë²ˆì§¸ë¡œ í° ë°°ì¶œêµ­ì´ë©°, ì£¼ë¡œ ìš´ì†¡ ë¶€ë¬¸ì—ì„œ ë†’ì€ ë¹„ì¤‘ì„ ì°¨ì§€í•©ë‹ˆë‹¤.", "US EPA Data Summary", 3),
        ("ìœ ëŸ½ ì—°í•©(EU)ì€ ì§€ë‚œ 10ë…„ê°„ ì¬ìƒ ì—ë„ˆì§€ ì •ì±… ë•ë¶„ì— ë°°ì¶œëŸ‰ì„ 20% ì´ìƒ ê°ì¶•í–ˆìŠµë‹ˆë‹¤.", "EU Green Deal Progress", 4),
        ("ê°€ì¥ ë¹ ë¥´ê²Œ ì„±ì¥í•˜ëŠ” ë°°ì¶œ ë¶€ë¬¸ì€ í•­ê³µ ìš´ì†¡ì´ë©°, íŠ¹íˆ êµ­ì œì„  ë¶€ë¬¸ì´ ê·¸ë ‡ìŠµë‹ˆë‹¤.", "Aviation Sector Analysis 2023", 5),
        ("ëŒ€í•œë¯¼êµ­ì˜ 2023ë…„ íƒ„ì†Œ ë°°ì¶œëŸ‰ì€ ì•½ 6ì–µ 2ì²œë§Œ í†¤ìœ¼ë¡œ, ì£¼ìš” ì‚°ì—…êµ­ ì¤‘ í•˜ë‚˜ì…ë‹ˆë‹¤.", "K-Emissions Data 2023", 6),
        ("ì‚°ì—… ë¶€ë¬¸(ì² ê°•, ì‹œë©˜íŠ¸)ì´ ì „ ì„¸ê³„ ë°°ì¶œëŸ‰ì˜ ì•½ 24%ë¥¼ ì°¨ì§€í•˜ëŠ” í•µì‹¬ ê°ì¶• ëŒ€ìƒì…ë‹ˆë‹¤.", "Industrial Decarbonization Report", 7),
        ("2050ë…„ ë„· ì œë¡œ ë‹¬ì„±ì„ ìœ„í•´ì„ , ì „ ì„¸ê³„ì ìœ¼ë¡œ ì—°ê°„ ìµœì†Œ 7.6%ì˜ ë°°ì¶œëŸ‰ ê°ì¶•ì´ í•„ìš”í•©ë‹ˆë‹¤.", "UN Climate Action Plan", 8),
    ]

    docs = [
        Document(page_content=content, metadata={"source": source, "page": page})
        for content, source, page in data_points
    ]
    
    st.info(f"âœ… íƒ„ì†Œ ë°°ì¶œ ë°ì´í„° í•µì‹¬ ì‚¬ì‹¤ {len(docs)}ê°œë¥¼ ë¡œë“œí–ˆìŠµë‹ˆë‹¤.")
    return docs

# 2. í…ìŠ¤íŠ¸ ì²­í¬ë“¤ì„ Chroma ì•ˆì— ì„ë² ë”© ë²¡í„°ë¡œ ì €ì¥
@st.cache_resource
def create_vector_store(_docs):
    """LangChain Documentsë¥¼ HuggingFace ì„ë² ë”© ëª¨ë¸ë¡œ Chromaì— ì €ì¥í•©ë‹ˆë‹¤."""
    # í•œêµ­ì–´ ì„ë² ë”© ëª¨ë¸ ì‚¬ìš© (ko-sroberta-multitask)
    embeddings = HuggingFaceEmbeddings(
        model_name="Huffon/kobigbird-roberta-base-finetuned-korquad"
    )
    
    # ê¸°ì¡´ Chroma DB í´ë”ë¥¼ ì‚¬ìš©í•˜ì§€ ì•Šê³  in-memoryë¡œ Chroma ìƒì„±
    vectorstore = Chroma.from_documents(documents=_docs, embedding=embeddings)
    return vectorstore

# 3. RAG ì²´ì¸ ì„¤ì • ë° ì´ˆê¸°í™”
@st.cache_resource(experimental_allow_widgets=True)
def initialize_components(selected_model):
    """LangChain RAG ì²´ì¸ì„ ì´ˆê¸°í™”í•˜ê³  ë°˜í™˜í•©ë‹ˆë‹¤."""

    # 1. ë°ì´í„° ë¡œë“œ ë° ë²¡í„° ì €ì¥ì†Œ ìƒì„±
    data_docs = load_and_split_data()
    vectorstore = create_vector_store(data_docs)
    retriever = vectorstore.as_retriever()

    # 2. ì±„íŒ… íˆìŠ¤í† ë¦¬ ìš”ì•½ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ (Contextualization)
    contextualize_q_system_prompt = """ì£¼ì–´ì§„ ëŒ€í™” ê¸°ë¡ê³¼ ì‚¬ìš©ì ì§ˆë¬¸ì„ ë°”íƒ•ìœ¼ë¡œ, \
    ëŒ€í™” ê¸°ë¡ ì—†ì´ë„ ì´í•´í•  ìˆ˜ ìˆëŠ” ë…ë¦½ì ì¸ ì§ˆë¬¸ìœ¼ë¡œ ë‹¤ì‹œ ì‘ì„±í•´ ì£¼ì„¸ìš”. \
    ì§ˆë¬¸ì— ì§ì ‘ ë‹µí•˜ì§€ ë§ê³ , í•„ìš”í•œ ê²½ìš°ì—ë§Œ ë‹¤ì‹œ ì‘ì„±í•˜ê³ , ê·¸ë ‡ì§€ ì•Šìœ¼ë©´ ì§ˆë¬¸ì„ ê·¸ëŒ€ë¡œ ë°˜í™˜í•˜ì„¸ìš”."""
    contextualize_q_prompt = ChatPromptTemplate.from_messages(
        [
            ("system", contextualize_q_system_prompt),
            MessagesPlaceholder("history"),
            ("human", "{input}"),
        ]
    )

    # 3. ì§ˆë¬¸-ë‹µë³€ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ (Data Analyst Persona)
    qa_system_prompt = """ë‹¹ì‹ ì€ **íƒ„ì†Œ ë°°ì¶œ ë°ì´í„° ë¶„ì„ê°€**ì…ë‹ˆë‹¤. \
    ì œê³µëœ ê²€ìƒ‰ëœ ì»¨í…ìŠ¤íŠ¸ ì¡°ê°(íƒ„ì†Œ ë°°ì¶œëŸ‰ ê´€ë ¨ ë°ì´í„°)ì„ ì‚¬ìš©í•˜ì—¬ ì§ˆë¬¸ì— ì •í™•í•˜ê²Œ ë‹µí•˜ì„¸ìš”. \
    ë°ì´í„°ê°€ í¬í•¨ë˜ì§€ ì•Šì€ ì¼ë°˜ì ì¸ ì§ˆë¬¸ì—ëŠ” ìƒì‹ ì„ ì—ì„œ ë‹µë³€í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. \
    ë¶„ì„ê°€ë¡œì„œì˜ ì „ë¬¸ì ì¸ ì–´ì¡°ë¥¼ ìœ ì§€í•˜ë©°, ë‹µë³€ì— ê´€ë ¨ ìˆ˜ì¹˜ë¥¼ ëª…í™•íˆ ì œì‹œí•´ ì£¼ì„¸ìš”. \
    ë§Œì•½ ë‹µë³€í•  ìˆ˜ ìˆëŠ” ë°ì´í„°ê°€ ë¶€ì¡±í•˜ë‹¤ë©´, 'ê´€ë ¨ ë°ì´í„°ê°€ ë¶€ì¡±í•˜ì—¬ ì •í™•í•œ ë¶„ì„ì„ ì œê³µí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.'ë¼ê³  ë§í•´ì£¼ì„¸ìš”.
    ëŒ€ë‹µì€ í•œêµ­ì–´ë¡œ í•˜ê³ , ì¡´ëŒ“ë§ì„ ì¨ì£¼ì„¸ìš”. ë‹µë³€ ë§ˆì§€ë§‰ì— ğŸ“Š ì´ëª¨ì§€ë¥¼ ì‚¬ìš©í•˜ì„¸ìš”.\

    {context}"""
    qa_prompt = ChatPromptTemplate.from_messages(
        [
            ("system", qa_system_prompt),
            MessagesPlaceholder("history"),
            ("human", "{input}"),
        ]
    )

    try:
        # Gemini-2.5-flash-preview-09-2025 ëª¨ë¸ ì‚¬ìš© ê·œì¹™ ì¤€ìˆ˜
        llm = ChatGoogleGenerativeAI(
            model="gemini-2.5-flash-preview-09-2025",
            temperature=0.7,
            convert_system_message_to_human=True
        )
    except Exception as e:
        st.error(f"âŒ Gemini ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {str(e)}")
        raise

    # 4. RAG ì²´ì¸ êµ¬ì„±
    history_aware_retriever = create_history_aware_retriever(llm, retriever, contextualize_q_prompt)
    question_answer_chain = create_stuff_documents_chain(llm, qa_prompt)
    rag_chain = create_retrieval_chain(history_aware_retriever, question_answer_chain)
    return rag_chain


# Streamlit UI
st.header("ğŸ“Š íƒ„ì†Œ ë°°ì¶œ ë°ì´í„° ë¶„ì„ê°€ ì±—ë´‡ ğŸŒ")
st.caption("ì œê³µëœ ê°€ìƒ íƒ„ì†Œ ë°°ì¶œ ë°ì´í„°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ë¶„ì„ ì§ˆë¬¸ì— ë‹µë³€í•©ë‹ˆë‹¤.")

# ëª¨ë¸ ì„ íƒ (ì„ íƒ ë°•ìŠ¤ëŠ” ì œê±°í•˜ê³ , ì½”ë“œëŠ” gemini-2.5-flash-preview-09-2025ë¥¼ ì‚¬ìš©í•˜ë„ë¡ ê³ ì •)
selected_model = "gemini-2.5-flash-preview-09-2025"
st.info(f"ì‚¬ìš© ëª¨ë¸: **{selected_model}**")

try:
    with st.spinner("ğŸ”§ íƒ„ì†Œ ë°ì´í„° ë¶„ì„ ì±—ë´‡ ì´ˆê¸°í™” ì¤‘..."):
        rag_chain = initialize_components(selected_model)
    st.success("âœ… ì±—ë´‡ì´ ì¤€ë¹„ë˜ì—ˆìŠµë‹ˆë‹¤! 2023ë…„ ê¸€ë¡œë²Œ íƒ„ì†Œ ë°°ì¶œ ë°ì´í„°ì— ëŒ€í•´ ì§ˆë¬¸í•´ ë³´ì„¸ìš”.")
except Exception as e:
    st.error(f"âš ï¸ ì´ˆê¸°í™” ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
    st.stop()

# Streamlit ì±„íŒ… ê¸°ë¡ ì„¤ì •
chat_history = StreamlitChatMessageHistory(key="chat_messages")

conversational_rag_chain = RunnableWithMessageHistory(
    rag_chain,
    lambda session_id: chat_history,
    input_messages_key="input",
    history_messages_key="history",
    output_messages_key="answer",
)

# ì´ˆê¸° í™˜ì˜ ë©”ì‹œì§€
if not chat_history.messages:
    chat_history.add_message(
        {"role": "assistant", 
         "content": "ì•ˆë…•í•˜ì„¸ìš”! ì €ëŠ” íƒ„ì†Œ ë°°ì¶œ ë°ì´í„° ë¶„ì„ê°€ì…ë‹ˆë‹¤. 2023ë…„ ê¸€ë¡œë²Œ íƒ„ì†Œ ë°°ì¶œëŸ‰ ì¶”ì •ì¹˜ì— ëŒ€í•´ ê¶ê¸ˆí•œ ì ì„ ì§ˆë¬¸í•´ ì£¼ì„¸ìš”. ì˜ˆë¥¼ ë“¤ì–´, 'ê°€ì¥ ë§ì´ ë°°ì¶œí•˜ëŠ” ë‚˜ë¼ëŠ” ì–´ë””ì¸ê°€ìš”?'ë¼ê³  ë¬¼ì–´ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤."}
    )

# ì±„íŒ… ê¸°ë¡ í‘œì‹œ
for msg in chat_history.messages:
    st.chat_message(msg.type).write(msg.content)


if prompt_message := st.chat_input("ë°ì´í„°ì— ëŒ€í•´ ì§ˆë¬¸í•˜ê¸°"):
    st.chat_message("human").write(prompt_message)
    
    # Streamlit API í˜¸ì¶œ ë° ì‘ë‹µ
    with st.chat_message("ai"):
        with st.spinner("ë°ì´í„° ë¶„ì„ ì¤‘..."):
            config = {"configurable": {"session_id": "any"}}
            response = conversational_rag_chain.invoke(
                {"input": prompt_message},
                config)
            
            answer = response['answer']
            st.write(answer)
            
            # ì°¸ê³  ë¬¸ì„œ í‘œì‹œ
            with st.expander("ì°¸ê³  ë°ì´í„° ì†ŒìŠ¤"):
                if 'context' in response:
                    for i, doc in enumerate(response['context']):
                        source = doc.metadata.get('source', 'ì•Œ ìˆ˜ ì—†ìŒ')
                        page = doc.metadata.get('page', 'N/A')
                        st.markdown(f"**[{i+1}] {source} (Page {page})**", help=doc.page_content)
                else:
                    st.markdown("ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ ë¬¸ì„œë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
