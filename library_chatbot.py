import os
import streamlit as st
import nest_asyncio

# Streamlitì—ì„œ ë¹„ë™ê¸° ì‘ì—…ì„ ìœ„í•œ ì´ë²¤íŠ¸ ë£¨í”„ ì„¤ì •
nest_asyncio.apply()

# LangChain components for RAG
from langchain_core.documents import Document
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_core.runnables.history import RunnableWithMessageHistory
from langchain.chains import create_retrieval_chain
from langchain.chains.combine_documents import create_stuff_documents_chain
from langchain.chains.history_aware_retriever import create_history_aware_retriever
from langchain_community.chat_message_histories.streamlit import StreamlitChatMessageHistory
from langchain_core.messages import AIMessage # AIMessage import ìœ ì§€

# docarray íŒ¨í‚¤ì§€ import ì‹œë„ ë° ì˜¤ë¥˜ ì²˜ë¦¬
try:
    # --- DocArrayInMemorySearchë¡œ ë³€ê²½ ---
    from langchain_community.vectorstores import DocArrayInMemorySearch
except ImportError:
    st.error(
        "âŒ **DocArrayInMemorySearch ì˜¤ë¥˜ ë°œìƒ**\n\n"
        "ì´ ê¸°ëŠ¥ì„ ì‚¬ìš©í•˜ë ¤ë©´ 'docarray' íŒ¨í‚¤ì§€ê°€ í•„ìš”í•©ë‹ˆë‹¤.\n\n"
        "**í•´ê²° ë°©ë²•:** ì‹¤í–‰ í™˜ê²½ì—ì„œ ë‹¤ìŒ ëª…ë ¹ì–´ë¥¼ ì‚¬ìš©í•˜ì—¬ íŒ¨í‚¤ì§€ë¥¼ ì„¤ì¹˜í•´ ì£¼ì„¸ìš”.\n"
        "```bash\npip install \"langchain[docarray]\"\n```"
    )
    st.stop()
except Exception as e:
    # ê¸°íƒ€ ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜ ì²˜ë¦¬
    st.error(f"âš ï¸ DocArrayInMemorySearch ë¡œë“œ ì¤‘ ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
    st.stop()


# Gemini API í‚¤ ì„¤ì • (Streamlit Secretsì—ì„œ ê°€ì ¸ì˜¤ê¸°)
try:
    # ì´ ë¶€ë¶„ì€ í™˜ê²½ì— ë”°ë¼ __initial_auth_tokenì„ ì‚¬ìš©í•˜ê±°ë‚˜, Streamlit secretsì—ì„œ GOOGLE_API_KEYë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.
    # ì‚¬ìš©ìì˜ ì›ë˜ ì½”ë“œë¥¼ ì¡´ì¤‘í•˜ì—¬ secrets ì‚¬ìš© êµ¬ë¬¸ì„ ìœ ì§€í•©ë‹ˆë‹¤.
    os.environ["GOOGLE_API_KEY"] = st.secrets["GOOGLE_API_KEY"]
except Exception as e:
    # ì´ í™˜ê²½ì—ì„œëŠ” API Keyê°€ ìë™ìœ¼ë¡œ ì£¼ì…ë˜ë¯€ë¡œ, ì˜¤ë¥˜ ëŒ€ì‹  ì•ˆë‚´ ë©”ì‹œì§€ë¥¼ í‘œì‹œí•©ë‹ˆë‹¤.
    st.info("ğŸ’¡ GOOGLE_API_KEYê°€ í™˜ê²½ ë³€ìˆ˜ë¡œë¶€í„° ìë™ ì„¤ì •ë©ë‹ˆë‹¤.")
    # ì‹¤ì œ Streamlit í™˜ê²½ì—ì„œëŠ” ì•„ë˜ ì½”ë“œë¥¼ ì‚¬ìš©í•˜ì—¬ í‚¤ ì„¤ì •ì„ ê°•ì œí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
    # st.error("âš ï¸ GOOGLE_API_KEYë¥¼ Streamlit Secretsì— ì„¤ì •í•´ì£¼ì„¸ìš”!")
    # st.stop()
    pass


# 3. RAG ì²´ì¸ ì„¤ì • ë° ì´ˆê¸°í™”
@st.cache_resource 
def initialize_components(selected_model):
    """LangChain RAG ì²´ì¸ì„ ì´ˆê¸°í™”í•˜ê³  ë°˜í™˜í•©ë‹ˆë‹¤."""

    # 1. ë°ì´í„° ë¡œë“œ ë° Document ìƒì„±
    data_points = [
        ("2023ë…„ ì „ ì„¸ê³„ ì´ ì´ì‚°í™”íƒ„ì†Œ ë°°ì¶œëŸ‰ì€ ì•½ 368ì–µ í†¤ìœ¼ë¡œ ì¶”ì •ë©ë‹ˆë‹¤.", "Global Emissions Report 2023", 1),
        ("ê°€ì¥ ë§ì€ íƒ„ì†Œë¥¼ ë°°ì¶œí•˜ëŠ” êµ­ê°€ëŠ” ì¤‘êµ­ì´ë©°, ì´ëŠ” ì „ ì„¸ê³„ ë°°ì¶œëŸ‰ì˜ ì•½ 31%ë¥¼ ì°¨ì§€í•©ë‹ˆë‹¤.", "IEA 2023 Review", 2),
        ("ë¯¸êµ­ì€ ë‘ ë²ˆì§¸ë¡œ í° ë°°ì¶œêµ­ì´ë©°, ì£¼ë¡œ ìš´ì†¡ ë¶€ë¬¸ì—ì„œ ë†’ì€ ë¹„ì¤‘ì„ ì°¨ì§€í•©ë‹ˆë‹¤.", "US EPA Data Summary", 3),
        ("ìœ ëŸ½ ì—°í•©(EU)ì€ ì§€ë‚œ 10ë…„ê°„ ì¬ìƒ ì—ë„ˆì§€ ì •ì±… ë•ë¶„ì— ë°°ì¶œëŸ‰ì„ 20% ì´ìƒ ê°ì¶•í–ˆìŠµë‹ˆë‹¤.", "EU Green Deal Progress", 4),
        ("ê°€ì¥ ë¹ ë¥´ê²Œ ì„±ì¥í•˜ëŠ” ë°°ì¶œ ë¶€ë¬¸ì€ í•­ê³µ ìš´ì†¡ì´ë©°, íŠ¹íˆ êµ­ì œì„  ë¶€ë¬¸ì´ ê·¸ë ‡ìŠµë‹ˆë‹¤.", "Aviation Sector Analysis 2023", 5),
        ("ëŒ€í•œë¯¼êµ­ì˜ 2023ë…„ íƒ„ì†Œ ë°°ì¶œëŸ‰ì€ ì•½ 6ì–µ 2ì²œë§Œ í†¤ìœ¼ë¡œ, ì£¼ìš” ì‚°ì—…êµ­ ì¤‘ í•˜ë‚˜ì…ë‹ˆë‹¤.", "K-Emissions Data 2023", 6),
        ("ì‚°ì—… ë¶€ë¬¸(ì² ê°•, ì‹œë©˜íŠ¸)ì´ ì „ ì„¸ê³„ ë°°ì¶œëŸ‰ì˜ ì•½ 24%ë¥¼ ì°¨ì§€í•˜ëŠ” í•µì‹¬ ê°ì¶• ëŒ€ìƒì…ë‹ˆë‹¤.", "Industrial Decarbonization Report", 7),
        ("2050ë…„ ë„· ì œë¡œ ë‹¬ì„±ì„ ìœ„í•´ì„ , ì „ ì„¸ê³„ì ìœ¼ë¡œ ì—°ê°„ ìµœì†Œ 7.6%ì˜ ë°°ì¶œëŸ‰ ê°ì¶•ì´ í•„ìš”í•©ë‹ˆë‹¤.", "UN Climate Action Plan", 8),
    ]

    data_docs = [
        Document(page_content=content, metadata={"source": source, "page": page})
        for content, source, page in data_points
    ]
    st.info(f"âœ… íƒ„ì†Œ ë°°ì¶œ ë°ì´í„° í•µì‹¬ ì‚¬ì‹¤ {len(data_docs)}ê°œë¥¼ ë¡œë“œí–ˆìŠµë‹ˆë‹¤.")
    
    # 2. ë²¡í„° ì €ì¥ì†Œ ìƒì„± (DocArrayInMemorySearch ì‚¬ìš©)
    # ì•ˆì •ì ì¸ ë‹¤êµ­ì–´ ì„ë² ë”© ëª¨ë¸ ì‚¬ìš©
    embeddings = HuggingFaceEmbeddings(
        model_name="sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2"
    )
    
    # DocArrayInMemorySearchë¥¼ ì‚¬ìš©í•˜ì—¬ ì•ˆì •ì ìœ¼ë¡œ ì¸ë©”ëª¨ë¦¬ ë²¡í„° ì €ì¥ì†Œë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
    vectorstore = DocArrayInMemorySearch.from_documents(documents=data_docs, embedding=embeddings)
    retriever = vectorstore.as_retriever()

    # 3. ì±„íŒ… íˆìŠ¤í† ë¦¬ ìš”ì•½ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ (Contextualization)
    contextualize_q_system_prompt = """ì£¼ì–´ì§„ ëŒ€í™” ê¸°ë¡ê³¼ ì‚¬ìš©ì ì§ˆë¬¸ì„ ë°”íƒ•ìœ¼ë¡œ, \
    ëŒ€í™” ê¸°ë¡ ì—†ì´ë„ ì´í•´í•  ìˆ˜ ìˆëŠ” ë…ë¦½ì ì¸ ì§ˆë¬¸ìœ¼ë¡œ ë‹¤ì‹œ ì‘ì„±í•´ ì£¼ì„¸ìš”. \
    ì§ˆë¬¸ì— ì§ì ‘ ë‹µí•˜ì§€ ë§ê³ , í•„ìš”í•œ ê²½ìš°ì—ë§Œ ë‹¤ì‹œ ì‘ì„±í•˜ê³ , ê·¸ë ‡ì§€ ì•Šìœ¼ë©´ ì§ˆë¬¸ì„ ê·¸ëŒ€ë¡œ ë°˜í™˜í•˜ì„¸ìš”."""
    contextualize_q_prompt = ChatPromptTemplate.from_messages(
        [
            ("system", contextualize_q_system_prompt),
            MessagesPlaceholder("history"),
            ("human", "{input}"),
        ]
    )

    # 4. ì§ˆë¬¸-ë‹µë³€ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ (Data Analyst Persona)
    qa_system_prompt = """ë‹¹ì‹ ì€ **íƒ„ì†Œ ë°°ì¶œ ë°ì´í„° ë¶„ì„ê°€**ì…ë‹ˆë‹¤. \
    ì œê³µëœ ê²€ìƒ‰ëœ ì»¨í…ìŠ¤íŠ¸ ì¡°ê°(íƒ„ì†Œ ë°°ì¶œëŸ‰ ê´€ë ¨ ë°ì´í„°)ì„ ì‚¬ìš©í•˜ì—¬ ì§ˆë¬¸ì— ì •í™•í•˜ê²Œ ë‹µí•˜ì„¸ìš”. \
    ë°ì´í„°ê°€ í¬í•¨ë˜ì§€ ì•Šì€ ì¼ë°˜ì ì¸ ì§ˆë¬¸ì—ëŠ” ìƒì‹ ì„ ì—ì„œ ë‹µë³€í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. \
    ë¶„ì„ê°€ë¡œì„œì˜ ì „ë¬¸ì ì¸ ì–´ì¡°ë¥¼ ìœ ì§€í•˜ë©°, ë‹µë³€ì— ê´€ë ¨ ìˆ˜ì¹˜ë¥¼ ëª…í™•íˆ ì œì‹œí•´ ì£¼ì„¸ìš”. \
    ë§Œì•½ ë‹µë³€í•  ìˆ˜ ìˆëŠ” ë°ì´í„°ê°€ ë¶€ì¡±í•˜ë‹¤ë©´, 'ê´€ë ¨ ë°ì´í„°ê°€ ë¶€ì¡±í•˜ì—¬ ì •í™•í•œ ë¶„ì„ì„ ì œê³µí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.'ë¼ê³  ë§í•´ì£¼ì„¸ìš”.
    ëŒ€ë‹µì€ í•œêµ­ì–´ë¡œ í•˜ê³ , ì¡´ëŒ“ë§ì„ ì¨ì£¼ì„¸ìš”. ë‹µë³€ ë§ˆì§€ë§‰ì— ğŸ“Š ì´ëª¨ì§€ë¥¼ ì‚¬ìš©í•˜ì„¸ìš”.\

    {context}"""
    qa_prompt = ChatPromptTemplate.from_messages(
        [
            ("system", qa_system_prompt),
            MessagesPlaceholder("history"),
            ("human", "{input}"),
        ]
    )

    try:
        # Gemini-2.5-flash-preview-09-2025 ëª¨ë¸ ì‚¬ìš© ê·œì¹™ ì¤€ìˆ˜
        llm = ChatGoogleGenerativeAI(
            model="gemini-2.5-flash-preview-09-2025",
            temperature=0.7,
            convert_system_message_to_human=True
        )
    except Exception as e:
        st.error(f"âŒ Gemini ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {str(e)}")
        raise

    # 5. RAG ì²´ì¸ êµ¬ì„±
    history_aware_retriever = create_history_aware_retriever(llm, retriever, contextualize_q_prompt)
    question_answer_chain = create_stuff_documents_chain(llm, qa_prompt)
    rag_chain = create_retrieval_chain(history_aware_retriever, question_answer_chain)
    return rag_chain


# Streamlit UI
st.header("ğŸ“Š íƒ„ì†Œ ë°°ì¶œ ë°ì´í„° ë¶„ì„ê°€ ì±—ë´‡ ğŸŒ")
st.caption("ì œê³µëœ ê°€ìƒ íƒ„ì†Œ ë°°ì¶œ ë°ì´í„°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ë¶„ì„ ì§ˆë¬¸ì— ë‹µë³€í•©ë‹ˆë‹¤.")

# ëª¨ë¸ ì„ íƒ (ì„ íƒ ë°•ìŠ¤ëŠ” ì œê±°í•˜ê³ , ì½”ë“œëŠ” gemini-2.5-flash-preview-09-2025ë¥¼ ì‚¬ìš©í•˜ë„ë¡ ê³ ì •)
selected_model = "gemini-2.5-flash-preview-09-2025"
st.info(f"ì‚¬ìš© ëª¨ë¸: **{selected_model}**")

try:
    with st.spinner("ğŸ”§ íƒ„ì†Œ ë°ì´í„° ë¶„ì„ ì±—ë´‡ ì´ˆê¸°í™” ì¤‘..."):
        # ì´ˆê¸°í™” í•¨ìˆ˜ê°€ ì´ì œ DocArrayInMemorySearchë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.
        rag_chain = initialize_components(selected_model) 
    st.success("âœ… ì±—ë´‡ì´ ì¤€ë¹„ë˜ì—ˆìŠµë‹ˆë‹¤! 2023ë…„ ê¸€ë¡œë²Œ íƒ„ì†Œ ë°°ì¶œ ë°ì´í„°ì— ëŒ€í•´ ì§ˆë¬¸í•´ ë³´ì„¸ìš”.")
except Exception as e:
    st.error(f"âš ï¸ ì´ˆê¸°í™” ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
    st.stop()

# Streamlit ì±„íŒ… ê¸°ë¡ ì„¤ì •
chat_history = StreamlitChatMessageHistory(key="chat_messages")

conversational_rag_chain = RunnableWithMessageHistory(
    rag_chain,
    lambda session_id: chat_history,
    input_messages_key="input",
    history_messages_key="history",
    output_messages_key="answer",
)

# ì´ˆê¸° í™˜ì˜ ë©”ì‹œì§€
if not chat_history.messages:
    # add_messageëŠ” LangChain BaseMessage ê°ì²´ë¥¼ ì¶”ê°€í•´ì•¼ í•©ë‹ˆë‹¤.
    chat_history.add_message(
        AIMessage(content="ì•ˆë…•í•˜ì„¸ìš”! ì €ëŠ” íƒ„ì†Œ ë°°ì¶œ ë°ì´í„° ë¶„ì„ê°€ì…ë‹ˆë‹¤. 2023ë…„ ê¸€ë¡œë²Œ íƒ„ì†Œ ë°°ì¶œëŸ‰ ì¶”ì •ì¹˜ì— ëŒ€í•´ ê¶ê¸ˆí•œ ì ì„ ì§ˆë¬¸í•´ ì£¼ì„¸ìš”. ì˜ˆë¥¼ ë“¤ì–´, 'ê°€ì¥ ë§ì´ ë°°ì¶œí•˜ëŠ” ë‚˜ë¼ëŠ” ì–´ë””ì¸ê°€ìš”?'ë¼ê³  ë¬¼ì–´ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
    )

# ì±„íŒ… ê¸°ë¡ í‘œì‹œ
for msg in chat_history.messages:
    # LangChain typeì„ Streamlit roleë¡œ ì•ˆì „í•˜ê²Œ ë³€í™˜í•©ë‹ˆë‹¤.
    if hasattr(msg, 'type'):
        # LangChain type ('human', 'ai')ì„ Streamlit role ('user', 'assistant')ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.
        role = "user" if msg.type == "human" else "assistant"
        st.chat_message(role).write(msg.content)
    # 'type' ì†ì„±ì´ ì—†ìœ¼ë©´, ì´ˆê¸° ë©”ì‹œì§€ì²˜ëŸ¼ ë‹¨ìˆœíˆ ë”•ì…”ë„ˆë¦¬ì¼ ê²½ìš°ë¥¼ ëŒ€ë¹„í•˜ì—¬ 'role'ì„ ì°¾ìŠµë‹ˆë‹¤.
    elif isinstance(msg, dict) and 'role' in msg:
        st.chat_message(msg['role']).write(msg.get('content', "ë©”ì‹œì§€ ë‚´ìš©ì„ ë¶ˆëŸ¬ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."))
    # ëª¨ë“  ì˜ˆì™¸ ìƒí™©ì— ëŒ€ë¹„í•œ ìµœí›„ì˜ ì²˜ë¦¬
    else:
        # ì˜¤ë¥˜ê°€ ë°œìƒí•œ ë©”ì‹œì§€ëŠ” "assistant"ë¡œ ê°€ì •í•˜ê³  ë‚´ìš©ì„ ë¬¸ìì—´ë¡œ í‘œì‹œí•©ë‹ˆë‹¤.
        st.chat_message("assistant").write(str(msg))


if prompt_message := st.chat_input("ë°ì´í„°ì— ëŒ€í•´ ì§ˆë¬¸í•˜ê¸°"):
    st.chat_message("human").write(prompt_message)
    
    # Streamlit API í˜¸ì¶œ ë° ì‘ë‹µ
    with st.chat_message("ai"):
        with st.spinner("ë°ì´í„° ë¶„ì„ ì¤‘..."):
            config = {"configurable": {"session_id": "any"}}
            response = conversational_rag_chain.invoke(
                {"input": prompt_message},
                config)
            
            answer = response['answer']
            st.write(answer)
            
            # ì°¸ê³  ë¬¸ì„œ í‘œì‹œ
            with st.expander("ì°¸ê³  ë°ì´í„° ì†ŒìŠ¤"):
                if 'context' in response:
                    for i, doc in enumerate(response['context']):
                        source = doc.metadata.get('source', 'ì•Œ ìˆ˜ ì—†ìŒ')
                        page = doc.metadata.get('page', 'N/A')
                        st.markdown(f"**[{i+1}] {source} (Page {page})**", help=doc.page_content)
                else:
                    st.markdown("ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ ë¬¸ì„œë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
