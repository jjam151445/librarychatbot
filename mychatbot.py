import os
import streamlit as st
import nest_asyncio

# Streamlitì—ì„œ ë¹„ë™ê¸° ì‘ì—…ì„ ìœ„í•œ ì´ë²¤íŠ¸ ë£¨í”„ ì„¤ì •
nest_asyncio.apply()

# LangChain components for RAG
from langchain_core.documents import Document
from langchain_community.document_loaders import PyPDFLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
# âš ï¸ ìˆ˜ì •: 'langchin_google_genai'ë¥¼ 'langchain_google_genai'ë¡œ ìˆ˜ì •
from langchain_google_genai import ChatGoogleGenerativeAI 
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_core.runnables.history import RunnableWithMessageHistory
from langchain.chains import create_retrieval_chain
from langchain.chains.combine_documents import create_stuff_documents_chain
from langchain.chains.history_aware_retriever import create_history_aware_retriever
from langchain_community.chat_message_histories.streamlit import StreamlitChatMessageHistory
from langchain_core.messages import AIMessage # AIMessage import ìœ ì§€

# SQLite/ChromaDB ìš°íšŒ ì½”ë“œ (ChromaDB ì‚¬ìš© ì‹œ í•„ìˆ˜)
__import__('pysqlite3')
import sys
sys.modules['sqlite3'] = sys.modules.pop('pysqlite3')
from langchain_chroma import Chroma


# Gemini API í‚¤ ì„¤ì •
try:
    # ì´ í™˜ê²½ì—ì„œëŠ” API Keyê°€ ìë™ ì£¼ì…ë˜ë¯€ë¡œ, ì˜¤ë¥˜ ëŒ€ì‹  ì•ˆë‚´ ë©”ì‹œì§€ë¥¼ í‘œì‹œí•©ë‹ˆë‹¤.
    # ì‚¬ìš©ìì˜ ì›ë˜ ì½”ë“œë¥¼ ì¡´ì¤‘í•˜ì—¬ secrets ì‚¬ìš© êµ¬ë¬¸ì„ ìœ ì§€í•©ë‹ˆë‹¤.
    os.environ["GOOGLE_API_KEY"] = st.secrets["GOOGLE_API_KEY"]
except Exception as e:
    st.info("ğŸ’¡ GOOGLE_API_KEYê°€ í™˜ê²½ ë³€ìˆ˜ë¡œë¶€í„° ìë™ ì„¤ì •ë©ë‹ˆë‹¤.")
    # st.error("âš ï¸ GOOGLE_API_KEYë¥¼ Streamlit Secretsì— ì„¤ì •í•´ì£¼ì„¸ìš”!")
    # st.stop()


# cache_resourceë¡œ í•œë²ˆ ì‹¤í–‰í•œ ê²°ê³¼ ìºì‹±í•´ë‘ê¸°
@st.cache_resource
def load_and_split_pdf(file_path):
    loader = PyPDFLoader(file_path)
    # PDFë¥¼ í˜ì´ì§€ ë‹¨ìœ„ë¡œ ë¡œë“œí•˜ê³  ë¶„í• 
    docs = loader.load_and_split()
    st.info(f"ğŸ“„ PDF ë¬¸ì„œì—ì„œ ì´ {len(docs)} í˜ì´ì§€ë¥¼ ë¡œë“œí–ˆìŠµë‹ˆë‹¤.")
    return docs

# í…ìŠ¤íŠ¸ ì²­í¬ë“¤ì„ Chroma ì•ˆì— ì„ë² ë”© ë²¡í„°ë¡œ ì €ì¥
@st.cache_resource
def create_vector_store(_docs):
    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
    split_docs = text_splitter.split_documents(_docs)
    st.info(f"ğŸ“„ {len(split_docs)}ê°œì˜ í…ìŠ¤íŠ¸ ì²­í¬ë¡œ ë¶„í• í–ˆìŠµë‹ˆë‹¤.")

    persist_directory = "./chroma_db"
    st.info("ğŸ¤– ì„ë² ë”© ëª¨ë¸ ë¡œë“œ ì¤‘... (jhgan/ko-sroberta-multitask)")
    embeddings = HuggingFaceEmbeddings(
        model_name="jhgan/ko-sroberta-multitask",
        model_kwargs={'device': 'cpu'},
        encode_kwargs={'normalize_embeddings': True}
    )

    st.info("ğŸ”¢ ë²¡í„° ì„ë² ë”© ìƒì„± ë° ì €ì¥ ì¤‘...")
    vectorstore = Chroma.from_documents(
        split_docs,
        embeddings,
        persist_directory=persist_directory
    )
    st.success("ğŸ’¾ ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ ìƒì„± ì™„ë£Œ!")
    return vectorstore

# ë§Œì•½ ê¸°ì¡´ì— ì €ì¥í•´ë‘” ChromaDBê°€ ìˆëŠ” ê²½ìš°, ì´ë¥¼ ë¡œë“œ
@st.cache_resource
def get_vectorstore(_docs):
    persist_directory = "./chroma_db"
    embeddings = HuggingFaceEmbeddings(
        model_name="jhgan/ko-sroberta-multitask",
        model_kwargs={'device': 'cpu'},
        encode_kwargs={'normalize_embeddings': True}
    )
    if os.path.exists(persist_directory):
        st.info("ğŸ”„ ê¸°ì¡´ ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ ë¡œë“œ ì¤‘...")
        return Chroma(
            persist_directory=persist_directory,
            embedding_function=embeddings
        )
    else:
        return create_vector_store(_docs)
    
# PDF ë¬¸ì„œ ë¡œë“œ-ë²¡í„° DB ì €ì¥-ê²€ìƒ‰ê¸°-íˆìŠ¤í† ë¦¬ ëª¨ë‘ í•©ì¹œ Chain êµ¬ì¶•
@st.cache_resource
def initialize_components(selected_model):
    # NOTE: ì´ íŒŒì¼ì€ Streamlit í™˜ê²½ì—ì„œ ì‹¤í–‰ë˜ë©°, 'íƒ„ì†Œ ë¶„ì„.pdf' íŒŒì¼ì´ ë¯¸ë¦¬ ì—…ë¡œë“œë˜ì–´ ìˆë‹¤ê³  ê°€ì •í•©ë‹ˆë‹¤.
    file_path = "íƒ„ì†Œ ë¶„ì„.pdf" 
    
    if not os.path.exists(file_path):
        st.error(f"âš ï¸ íŒŒì¼ ê²½ë¡œ ì˜¤ë¥˜: '{file_path}' íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. íŒŒì¼ì„ ì—…ë¡œë“œí•˜ê±°ë‚˜ ê²½ë¡œë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”.")
        st.stop()
        
    pages = load_and_split_pdf(file_path)
    vectorstore = get_vectorstore(pages)
    retriever = vectorstore.as_retriever()

    # 3. ì±„íŒ… íˆìŠ¤í† ë¦¬ ìš”ì•½ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ (ê¸°ì¡´ ì˜ì–´ í”„ë¡¬í”„íŠ¸ ìœ ì§€)
    contextualize_q_system_prompt = """Given a chat history and the latest user question \
    which might reference context in the chat history, formulate a standalone question \
    which can be understood without the chat history. Do NOT answer the question, \
    just reformulate it if needed and otherwise return it as is."""
    contextualize_q_prompt = ChatPromptTemplate.from_messages(
        [
            ("system", contextualize_q_system_prompt),
            MessagesPlaceholder("history"),
            ("human", "{input}"),
        ]
    )

    # 4. ì§ˆë¬¸-ë‹µë³€ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ (âœ¨ íƒ„ì†Œ ë°°ì¶œ ì „ë¬¸ê°€ í˜ë¥´ì†Œë‚˜ ê°•í™”)
    qa_system_prompt = """ë‹¹ì‹ ì€ **íƒ„ì†Œ ë°°ì¶œ ë° í™˜ê²½ ë¶„ì„ ì „ë¬¸ê°€**ì…ë‹ˆë‹¤. \
    ì œê³µëœ ê²€ìƒ‰ëœ ì»¨í…ìŠ¤íŠ¸ ì¡°ê°(PDF ë¬¸ì„œ ë‚´ìš©)ì„ ì‚¬ìš©í•˜ì—¬ ì§ˆë¬¸ì— ê¹Šì´ ìˆê³  ì •í™•í•˜ê²Œ ë‹µí•˜ì„¸ìš”. \
    ì „ë¬¸ê°€ë¡œì„œì˜ ì‹ ë¢°ê°ì„ ì£¼ëŠ” ì–´ì¡°ë¥¼ ì‚¬ìš©í•˜ê³ , ë‹µë³€ ì‹œ ê´€ë ¨ëœ ì‚¬ì‹¤ê³¼ ìˆ˜ì¹˜, í˜¹ì€ ë¬¸ì„œì˜ í•µì‹¬ ë‚´ìš©ì„ ëª…í™•íˆ ì œì‹œí•´ ì£¼ì„¸ìš”. \
    ë§Œì•½ ë‹µë³€í•  ìˆ˜ ìˆëŠ” ì •ë³´ê°€ ë¶€ì¡±í•˜ë‹¤ë©´, 'ì œê³µëœ ë¬¸ì„œ ë‚´ì—ì„œëŠ” í•´ë‹¹ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.'ë¼ê³  ì •ì¤‘í•˜ê²Œ ë§í•´ì£¼ì„¸ìš”. \
    ëª¨ë“  ëŒ€ë‹µì€ í•œêµ­ì–´ë¡œ í•˜ê³ , ì¡´ëŒ“ë§ì„ ì‚¬ìš©í•´ ì£¼ì„¸ìš”. ë‹µë³€ ë§ˆì§€ë§‰ì—ëŠ” í•­ìƒ ğŸŒ¿ ì´ëª¨ì§€ë¥¼ ë„£ì–´ì£¼ì„¸ìš”.\

    {context}"""
    qa_prompt = ChatPromptTemplate.from_messages(
        [
            ("system", qa_system_prompt),
            MessagesPlaceholder("history"),
            ("human", "{input}"),
        ]
    )

    try:
        # ëª¨ë¸ëª…ì€ option ë³€ìˆ˜ë¥¼ í†µí•´ ì „ë‹¬ë°›ìŠµë‹ˆë‹¤.
        llm = ChatGoogleGenerativeAI(
            model=selected_model,
            temperature=0.3, # ì „ë¬¸ì ì¸ ë‹µë³€ì„ ìœ„í•´ ì˜¨ë„ë¥¼ ì•½ê°„ ë‚®ì¶¥ë‹ˆë‹¤.
            convert_system_message_to_human=True
        )
    except Exception as e:
        # ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨ ì‹œ ëª…í™•í•œ ì˜¤ë¥˜ ë©”ì‹œì§€ë¥¼ ì¶œë ¥í•©ë‹ˆë‹¤.
        st.error(f"âŒ Gemini ëª¨ë¸ '{selected_model}' ë¡œë“œ ì‹¤íŒ¨: {str(e)}")
        st.info("ğŸ’¡ ëª¨ë¸ëª…ì´ ì˜¬ë°”ë¥¸ì§€ í™•ì¸í•´ì£¼ì„¸ìš”. (ì˜ˆ: gemini-2.5-flash)")
        raise

    # 5. RAG ì²´ì¸ êµ¬ì„±
    history_aware_retriever = create_history_aware_retriever(llm, retriever, contextualize_q_prompt)
    question_answer_chain = create_stuff_documents_chain(llm, qa_prompt)
    rag_chain = create_retrieval_chain(history_aware_retriever, question_answer_chain)
    return rag_chain

# Streamlit UI
st.header("ğŸŒ¿ PDF ê¸°ë°˜ íƒ„ì†Œ ë°°ì¶œ ë¶„ì„ ì „ë¬¸ê°€ ì±—ë´‡")
st.caption("ì—…ë¡œë“œëœ 'íƒ„ì†Œ ë¶„ì„.pdf' ë¬¸ì„œë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì •í™•í•œ ë‹µë³€ì„ ì œê³µí•©ë‹ˆë‹¤.")

# Gemini ëª¨ë¸ ì„ íƒ
option = st.selectbox("Select Gemini Model",
    ("gemini-2.5-flash", "gemini-2.5-pro", "gemini-2.0-flash-exp"),
    index=0,
    help="Gemini 2.5 Flashê°€ ê°€ì¥ ë¹ ë¥´ê³  íš¨ìœ¨ì ì…ë‹ˆë‹¤"
)

try:
    # íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ëŠ” ê²½ìš°ë¥¼ ëŒ€ë¹„í•˜ì—¬ initialize_componentsì—ì„œ ì²˜ë¦¬
    with st.spinner("ğŸ”§ ì±—ë´‡ ì´ˆê¸°í™” ë° PDF ë¶„ì„ ì¤‘... (ì²« ì‹¤í–‰ ì‹œ ì‹œê°„ì´ ì˜¤ë˜ ê±¸ë¦½ë‹ˆë‹¤)"):
        rag_chain = initialize_components(option)
    st.success("âœ… ì±—ë´‡ ì´ˆê¸°í™” ì™„ë£Œ! ì´ì œ ì§ˆë¬¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
except Exception as e:
    # initialize_components ë‚´ì—ì„œ st.stop()ì´ í˜¸ì¶œë˜ì§€ ì•Šë„ë¡ ìˆ˜ì •í•˜ì—¬ ì˜¤ë¥˜ë¥¼ ëª…í™•íˆ ë³´ì—¬ì¤ë‹ˆë‹¤.
    st.error(f"âš ï¸ ì±—ë´‡ ì´ˆê¸°í™” ì‹¤íŒ¨. ì˜¤ë¥˜: {str(e)}")
    # st.info("PDF íŒŒì¼ ê²½ë¡œì™€ API í‚¤ë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”.") # initialize_componentsì—ì„œ ì´ë¯¸ ì²˜ë¦¬ë¨
    st.stop()


chat_history = StreamlitChatMessageHistory(key="chat_messages")

conversational_rag_chain = RunnableWithMessageHistory(
    rag_chain,
    lambda session_id: chat_history,
    input_messages_key="input",
    history_messages_key="history",
    output_messages_key="answer",
)

# ì´ˆê¸° í™˜ì˜ ë©”ì‹œì§€ (Session State ëŒ€ì‹  StreamlitChatMessageHistory ì‚¬ìš©)
if not chat_history.messages:
    chat_history.add_message(
        AIMessage(content="ì•ˆë…•í•˜ì„¸ìš”! ì €ëŠ” íƒ„ì†Œ ë°°ì¶œ ë° í™˜ê²½ ë¶„ì„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì–´ë–¤ ì§ˆë¬¸ì´ë“  ê¹Šì´ ìˆê²Œ ë‹µë³€í•´ ë“œë¦¬ê² ìŠµë‹ˆë‹¤. ğŸŒ¿")
    )

# ì±„íŒ… ê¸°ë¡ í‘œì‹œ (âš ï¸ ìˆ˜ì •: msg.type ëŒ€ì‹  ì•ˆì „í•œ ì—­í•  ë³€í™˜ ë¡œì§ ì‚¬ìš©)
for msg in chat_history.messages:
    # LangChain BaseMessage ê°ì²´ë¥¼ Streamlit Roleë¡œ ë³€í™˜
    if hasattr(msg, 'type'):
        role = "user" if msg.type == "human" else "assistant"
        st.chat_message(role).write(msg.content)
    else:
        # ì•ˆì „ ì¥ì¹˜: êµ¬ì¡°ê°€ ì˜ˆìƒê³¼ ë‹¤ë¥¸ ê²½ìš° ë©”ì‹œì§€ë¥¼ ê·¸ëŒ€ë¡œ ì¶œë ¥í•©ë‹ˆë‹¤.
        st.chat_message("assistant").write(str(msg.content))


if prompt_message := st.chat_input("ì „ë¬¸ê°€ì—ê²Œ ì§ˆë¬¸í•˜ê¸°"):
    st.chat_message("user").write(prompt_message) # human ëŒ€ì‹  user ì—­í•  ì‚¬ìš©
    
    with st.chat_message("assistant"): # ai ëŒ€ì‹  assistant ì—­í•  ì‚¬ìš©
        with st.spinner("ë¬¸ì„œ ë¶„ì„ ë° ë‹µë³€ ìƒì„± ì¤‘..."):
            config = {"configurable": {"session_id": "any"}}
            response = conversational_rag_chain.invoke(
                {"input": prompt_message},
                config)
            
            answer = response['answer']
            st.write(answer)
            
            with st.expander("ì°¸ê³  ë¬¸ì„œ í™•ì¸"):
                if 'context' in response:
                    for i, doc in enumerate(response['context']):
                        source = doc.metadata.get('source', 'ì•Œ ìˆ˜ ì—†ìŒ')
                        page = doc.metadata.get('page', 'N/A')
                        st.markdown(f"**[{i+1}] ì¶œì²˜: {source}** (í˜ì´ì§€: {page})", help=doc.page_content)
                else:
                    st.markdown("ë‹µë³€ì— ì‚¬ìš©ëœ ë¬¸ì„œ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
